
@online{GazeboFuel-OpenRobotics-Black-and-Decker-Cordless-Drill,
	title={Black and Decker Cordless Drill},
	organization={Open Robotics},
	date={2023},
	month={September},
	day={3},
	author={OpenRobotics},
	url={https://fuel.gazebosim.org/1.0/OpenRobotics/models/Black and Decker Cordless Drill},
}

@misc{noauthor_stanford_nodate,
	title = {The {Stanford} {3D} {Scanning} {Repository}},
	url = {https://graphics.stanford.edu/data/3Dscanrep/},
	urldate = {2025-02-25},
	file = {The Stanford 3D Scanning Repository:/home/ian/Zotero/storage/S2SJKIAA/3Dscanrep.html:text/html},
}

@online{GazeboFuel-OpenRobotics-Large-Crate,
	title={Large Crate},
	organization={Open Robotics},
	date={2023},
	month={September},
	day={3},
	author={OpenRobotics},
	url={https://fuel.gazebosim.org/1.0/OpenRobotics/models/Large Crate},
}

@online{GazeboFuel-OpenRobotics-Small-Blue-Box,
	title={Small Blue Box},
	organization={Open Robotics},
	date={2023},
	month={September},
	day={3},
	author={OpenRobotics},
	url={https://fuel.gazebosim.org/1.0/OpenRobotics/models/Small Blue Box},
}
@article{qi2021review,
  title={Review of multi-view 3D object recognition methods based on deep learning},
  author={Qi, Shaohua and Ning, Xin and Yang, Guowei and Zhang, Liping and Long, Peng and Cai, Weiwei and Li, Weijun},
  journal={Displays},
  volume={69},
  pages={102053},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{su2015multi,
  title={Multi-view convolutional neural networks for 3d shape recognition},
  author={Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={945--953},
  year={2015}
}

@article{loncomilla2016object,
  title={Object recognition using local invariant features for robotic applications: A survey},
  author={Loncomilla, Patricio and Ruiz-del-Solar, Javier and Mart{\'\i}nez, Luz},
  journal={Pattern Recognition},
  volume={60},
  pages={499--514},
  year={2016},
  publisher={Elsevier}
}

@article{elfes1989using,
  title={Using occupancy grids for mobile robot perception and navigation},
  author={Elfes, Alberto},
  journal={Computer},
  volume={22},
  number={6},
  pages={46--57},
  year={1989},
  publisher={IEEE}
}

@book{meagher1980octree,
  title={Octree encoding: A new technique for the representation, manipulation and display of arbitrary 3-d objects by computer},
  author={Meagher, Donald JR},
  year={1980},
  publisher={Electrical and Systems Engineering Department Rensseiaer Polytechnic~…}
}

@article{museth2013vdb,
  title={VDB: High-resolution sparse volumes with dynamic topology},
  author={Museth, Ken},
  journal={ACM transactions on graphics (TOG)},
  volume={32},
  number={3},
  pages={1--22},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@misc{noauthor_openvdb_nodate,
	title = {{OpenVDB} - {About}},
	url = {https://www.openvdb.org/about/},
	urldate = {2025-01-12},
	file = {OpenVDB - About:/home/ian/Zotero/storage/VAB3Y3PS/about.html:text/html},
}

@misc{name_robotis_nodate,
	title = {{ROBOTIS} e-{Manual}},
	url = {https://emanual.robotis.com/docs/en/platform/turtlebot3/overview/},
	abstract = {e-Manual wiki},
	language = {en},
	urldate = {2025-01-10},
	journal = {ROBOTIS e-Manual},
}

@inproceedings{vasquez-gomez_hierarchical_2013,
	title = {Hierarchical {Ray} {Tracing} for {Fast} {Volumetric} {Next}-{Best}-{View} {Planning}},
	url = {https://ieeexplore.ieee.org/document/6569201/?arnumber=6569201&tag=1},
	doi = {10.1109/CRV.2013.42},
	abstract = {A mobile robot must have the ability of building a representation of its environment and the objects in it. To build a three-dimensional (3D) model of a physical object, several scans must be taken at different locations. Selecting each location is the next-best-view problem. Search based methods, where candidate views are generated and evaluated by a utility function, are a solution. However, such methods are slow for high resolution models given that the evaluation requires visibility computation in 3D. We propose a scene representation by octrees with a hierarchical ray tracing that reduces the visibility computation time. Such method performs a coarse ray tracing, except for the interesting volumes where a finer resolution is applied. The method decreases the computation time at least one order of magnitude. Saving time with this method leads to evaluate more constraints and more candidate views in high resolution models.},
	urldate = {2025-01-10},
	booktitle = {2013 {International} {Conference} on {Computer} and {Robot} {Vision}},
	author = {Vasquez-Gomez, J. Irving and Sucar, L. Enrique and Murrieta-Cid, Rafael},
	month = may,
	year = {2013},
	keywords = {Equations, Image reconstruction, Octrees, Planning, Ray tracing, Robot sensing systems, mapping, next best view, object modeling, view planning},
	pages = {181--187},
}

@misc{noauthor_nav2_nodate,
	title = {Nav2 — {Nav2} 1.0.0 documentation},
	url = {https://docs.nav2.org/index.html},
	urldate = {2024-12-31},
}

@misc{noauthor_difference_2024,
	title = {Difference between {URDF} and {SDF} and how to convert {\textbar} {Introduction} to {ROS2} and {Robotics}},
	url = {https://www.learnros2.com/ros/tech-blog/difference-between-urdf-and-sdf-and-how-to-convert},
	language = {en},
	urldate = {2024-12-26},
	month = jan,
	year = {2024},
}

@inproceedings{da_silva_lubanco_novel_2020,
	title = {A {Novel} {Frontier}-{Based} {Exploration} {Algorithm} for {Mobile} {Robots}},
	url = {https://ieeexplore.ieee.org/document/9064866/?arnumber=9064866},
	doi = {10.1109/ICMRE49073.2020.9064866},
	abstract = {This paper aims to bring a novel approach to the exploration paradigm of mobile robots. Consequently, it uses the frontier-exploration method alongside a utility function in order to determine new goals to be achieved by the robot. The proposed approach was implemented using the Robot Operating System as the middleware, and makes use of several packages for, e.g., mapping and navigation. The algorithm implemented in this paper was motivated by the original work on frontier exploration developed by Yamauchi [1] as well as the more recent development, e.g., histogram-based frontier exploration. In addition, this paper aims to include additional parameters in order to enhance the decisions made by the exploration algorithm.},
	urldate = {2024-12-23},
	booktitle = {2020 6th {International} {Conference} on {Mechatronics} and {Robotics} {Engineering} ({ICMRE})},
	author = {da Silva Lubanco, Daniel Louback and Pichler-Scheder, Markus and Schlechter, Thomas},
	month = feb,
	year = {2020},
	keywords = {Clustering algorithms, Mobile robots, Navigation, Robot kinematics, Robot sensing systems, exploration, mapping, navigation, robotics},
	pages = {1--5},
}

@article{batinovic_multi-resolution_2021,
	title = {A {Multi}-{Resolution} {Frontier}-{Based} {Planner} for {Autonomous} {3D} {Exploration}},
	volume = {6},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/9387089/?arnumber=9387089},
	doi = {10.1109/LRA.2021.3068923},
	abstract = {In this letter we propose a planner for 3D exploration that is suitable for applications using state-of-the-art 3D sensors, such as LiDARs, that produce large point clouds with each scan. The planner is based on the detection of a frontier - a boundary between the explored and the unknown part of the environment - and consists of the algorithm for detecting frontier points, followed by the clustering of frontier points and the selection of the best frontier point to be explored. Compared to existing frontier-based approaches, the planner is more scalable, i.e., it requires less time for the same environment size while ensuring similar exploration time. The performance is achieved by relying not on data obtained directly from the 3D sensor, but on data obtained by a mapping algorithm. In order to cluster the frontier points, we exploit the properties of the Octree environment representation, which allows easy analysis with different resolutions. The planner is tested in the simulation environment and in an outdoor test area with a UAV equipped with a LiDAR sensor. The results show the advantages of the approach compared to current state-of-the-art approaches.},
	number = {3},
	urldate = {2024-12-23},
	journal = {IEEE Robotics and Automation Letters},
	author = {Batinovic, Ana and Petrovic, Tamara and Ivanovic, Antun and Petric, Frano and Bogdan, Stjepan},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Aerial systems, Cloud computing, Laser radar, Navigation, Sensors, Simultaneous localization and mapping, Three-dimensional displays, Two dimensional displays, autonomous agents, perception and autonomy},
	pages = {4528--4535},
}

@inproceedings{yamauchi_frontier-based_1997,
	title = {A frontier-based approach for autonomous exploration},
	url = {https://ieeexplore.ieee.org/document/613851/?arnumber=613851},
	doi = {10.1109/CIRA.1997.613851},
	abstract = {We introduce a new approach for exploration based on the concept of frontiers, regions on the boundary between open space and unexplored space. By moving to new frontiers, a mobile robot can extend its map into new territory until the entire environment has been explored. We describe a method for detecting frontiers in evidence grids and navigating to these frontiers. We also introduce a technique for minimizing specular reflections in evidence grids using laser-limited sonar. We have tested this approach with a real mobile robot, exploring real-world office environments cluttered with a variety of obstacles. An advantage of our approach is its ability to explore both large open spaces and narrow cluttered spaces, with walls and obstacles in arbitrary orientation.},
	urldate = {2024-12-23},
	booktitle = {Proceedings 1997 {IEEE} {International} {Symposium} on {Computational} {Intelligence} in {Robotics} and {Automation} {CIRA}'97. '{Towards} {New} {Computational} {Principles} for {Robotics} and {Automation}'},
	author = {Yamauchi, B.},
	month = jul,
	year = {1997},
	keywords = {Artificial intelligence, Humans, Indoor environments, Laboratories, Mobile robots, Orbital robotics, Sonar navigation, Space exploration, Testing},
	pages = {146--151},
}

@article{topiwala_frontier_nodate,
	title = {Frontier {Based} {Exploration} for {Autonomous} {Robot}},
	abstract = {Exploration is process of selecting target points that yield the biggest contribution to a specific gain function at an initially unknown environment. Frontier-based exploration is the most common approach to exploration, wherein frontiers are regions on the boundary between open space and unexplored space. By moving to a new frontier, we can keep building the map of the environment, until there are no new frontiers left to detect. In this paper, an autonomous frontier-based exploration strategy, namely Wavefront Frontier Detector (WFD) is described and implemented on Gazebo Simulation Environment as well as on hardware platform, i.e. Kobuki TurtleBot using Robot Operating System (ROS). The advantage of this algorithm is that the robot can explore large open spaces as well as small cluttered spaces. Further, the map generated from this technique is compared and validated with the map generated using turtlebot\_teleop ROS Package.},
	language = {en},
	author = {Topiwala, Anirudh and Inani, Pranav and Kathpal, Abhishek},
}

@article{batinovic_multi-resolution_2021-1,
	title = {A {Multi}-{Resolution} {Frontier}-{Based} {Planner} for {Autonomous} {3D} {Exploration}},
	volume = {6},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/9387089/?arnumber=9387089},
	doi = {10.1109/LRA.2021.3068923},
	abstract = {In this letter we propose a planner for 3D exploration that is suitable for applications using state-of-the-art 3D sensors, such as LiDARs, that produce large point clouds with each scan. The planner is based on the detection of a frontier - a boundary between the explored and the unknown part of the environment - and consists of the algorithm for detecting frontier points, followed by the clustering of frontier points and the selection of the best frontier point to be explored. Compared to existing frontier-based approaches, the planner is more scalable, i.e., it requires less time for the same environment size while ensuring similar exploration time. The performance is achieved by relying not on data obtained directly from the 3D sensor, but on data obtained by a mapping algorithm. In order to cluster the frontier points, we exploit the properties of the Octree environment representation, which allows easy analysis with different resolutions. The planner is tested in the simulation environment and in an outdoor test area with a UAV equipped with a LiDAR sensor. The results show the advantages of the approach compared to current state-of-the-art approaches.},
	number = {3},
	urldate = {2024-12-11},
	journal = {IEEE Robotics and Automation Letters},
	author = {Batinovic, Ana and Petrovic, Tamara and Ivanovic, Antun and Petric, Frano and Bogdan, Stjepan},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Aerial systems, Cloud computing, Laser radar, Navigation, Sensors, Simultaneous localization and mapping, Three-dimensional displays, Two dimensional displays, autonomous agents, perception and autonomy},
	pages = {4528--4535},
}

@article{banta_next-best-view_2000,
	title = {A next-best-view system for autonomous 3-{D} object reconstruction},
	volume = {30},
	issn = {1558-2426},
	url = {https://ieeexplore.ieee.org/document/867866/?arnumber=867866},
	doi = {10.1109/3468.867866},
	abstract = {The focus of this paper is to design and implement a system capable of automatically reconstructing a prototype 3D model from a minimum number of range images of an object. Given an ideal 3D object model, the system iteratively renders range and intensity images of the model from a specified position, assimilates the range information into a prototype model, and determines the sensor pose (position and orientation) from which an optimal amount of previously unrecorded information may be acquired. Reconstruction is terminated when the model meets a given threshold of accuracy. Such a system has applications in the context of robot navigation, manufacturing, or hazardous materials handling. The system has been tested successfully on several synthetic data models, and each set of results was found to be reasonably consistent with an intuitive human search. The number of views necessary to reconstruct an adequate 3D prototype depends on the complexity of the object or scene and the initial data collected. The prototype models which the system recovers compare well with the ideal models.},
	number = {5},
	urldate = {2024-11-22},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {Banta, J.E. and Wong, L.R. and Dumont, C. and Abidi, M.A.},
	month = sep,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	keywords = {Focusing, Hazardous materials, Image reconstruction, Image sensors, Manufacturing, Navigation, Prototypes, Rendering (computer graphics), Robot sensing systems, Sensor systems},
	pages = {589--598},
}

@article{torabi_autonomous_2012,
	title = {An autonomous six-{DOF} eye-in-hand system for in situ {3D} object modeling},
	volume = {31},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364911425836},
	doi = {10.1177/0278364911425836},
	abstract = {We present an integrated and fully autonomous eye-in-hand system for 3D object modeling. The system hardware consists of a laser range scanner mounted on a six-DOF manipulator arm and the task is to autonomously build a 3D model of an object in situ where the object may not be moved and must be scanned in its original location. Our system assumes no knowledge of object shape or geometry other than that it is within a bounding box whose location and size are known a priori, and, furthermore, the environment is unknown. The overall planner integrates the three main algorithms in the system: one that finds the next best view (NBV) for modeling the object; one that finds the NBV for exploration, i.e. exploring the environment, so the arm can move to the modeling view pose; and finally a sensor-based path planner, that is able to find a collision-free path to the view configuration determined by either of the the two view planners. Our modeling NBV algorithm efficiently searches the five-dimensional view space to determine the best modeling viewpoint, while considering key constraints such as field of view (FOV), overlap, and occlusion. If the determined viewpoint is reachable, the sensor-based path planner determines a collision-free path to move the manipulator to the desired view configuration, and a scan of the object is taken. Since the workspace is initially unknown, in some phases, the exploration view planner is used to increase information about the reachability and also the status of the modeling view configurations, since the view configuration may lie in an unknown workspace. This is repeated until the object modeling is complete or the planner deems that no further progress can be made, and the system stops. We have implemented the system with a six-DOF powercube arm and a wrist mounted Hokuyo URG-04LX laser scanner. Our results show that the system is able to autonomously build a 3D model of an object in situ in an unknown environment.},
	language = {en},
	number = {1},
	urldate = {2024-10-23},
	journal = {The International Journal of Robotics Research},
	author = {Torabi, Liila and Gupta, Kamal},
	month = jan,
	year = {2012},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {82--100},
}

@inproceedings{grosse_besselmann_vdb-mapping_2021,
	title = {{VDB}-{Mapping}: {A} {High} {Resolution} and {Real}-{Time} {Capable} {3D} {Mapping} {Framework} for {Versatile} {Mobile} {Robots}},
	shorttitle = {{VDB}-{Mapping}},
	doi = {10.1109/CASE49439.2021.9551430},
	author = {Grosse Besselmann, Marvin and Puck, Lennart and Steffen, Lea and Roennau, Arne and Dillmann, Rüdiger},
	month = aug,
	year = {2021},
	pages = {448--454},
}

@misc{vasquez_irvingvasquezvpl_2024,
	title = {irvingvasquez/vpl},
	copyright = {BSD-3-Clause},
	url = {https://github.com/irvingvasquez/vpl},
	abstract = {View planning library},
	urldate = {2024-10-21},
	author = {Vasquez, Irving},
	month = jan,
	year = {2024},
	note = {original-date: 2017-01-20T01:01:18Z},
}

@article{hornung_octomap_2013,
	title = {{OctoMap}: an efficient probabilistic {3D} mapping framework based on octrees},
	volume = {34},
	copyright = {http://www.springer.com/tdm},
	issn = {0929-5593, 1573-7527},
	shorttitle = {{OctoMap}},
	url = {http://link.springer.com/10.1007/s10514-012-9321-0},
	doi = {10.1007/s10514-012-9321-0},
	abstract = {Three-dimensional models provide a volumetric representation of space which is important for a variety of robotic applications including ﬂying robots and robots that are equipped with manipulators. In this paper, we present an open-source framework to generate volumetric 3D environment models. Our mapping approach is based on octrees and uses probabilistic occupancy estimation. It explicitly represents not only occupied space, but also free and unknown areas. Furthermore, we propose an octree map compression method that keeps the 3D models compact. Our framework is available as an open-source C++ library and has already been successfully applied in several robotics projects. We present a series of experimental results carried out with real robots and on publicly available real-world datasets. The results demonstrate that our approach is able to update the representation efﬁciently and models the data consistently while keeping the memory requirement at a minimum.},
	language = {en},
	number = {3},
	urldate = {2024-10-17},
	journal = {Autonomous Robots},
	author = {Hornung, Armin and Wurm, Kai M. and Bennewitz, Maren and Stachniss, Cyrill and Burgard, Wolfram},
	month = apr,
	year = {2013},
	pages = {189--206},
}

@article{pan_scvp_2022,
	title = {{SCVP}: {Learning} {One}-{Shot} {View} {Planning} via {Set} {Covering} for {Unknown} {Object} {Reconstruction}},
	volume = {7},
	issn = {2377-3766},
	shorttitle = {{SCVP}},
	url = {https://ieeexplore.ieee.org/document/9670705/?arnumber=9670705},
	doi = {10.1109/LRA.2022.3140449},
	abstract = {The view planning (VP) problem in robotic active vision enables a robot system to automatically perform object reconstruction tasks. Lacking prior knowledge, next-best-view (NBV) methods are typically used to plan a view sequence, with the goal of covering as many object surface areas as possible in an unknown environment. However, such methods have two problems: (1) they are unable to perform global path planning; and (2) the reconstruction process is inefficient because of time-consuming ray casting and high movement cost. We propose a neural network, SCVP, to pre-learn prior knowledge via set covering (SC) based training so as to achieve one-shot view planning. The SCVP network takes the volumetric occupancy grid as input and directly predicts a small (ideally minimum) number of views that cover all surface areas. Given object 3D models as a priori geometric knowledge, the training dataset is automatically labeled by the set covering optimization method. We propose a global path planning method to reconstruct objects without redundant movement. Comparative experiments on multiple datasets of 3D models show that, under the condition of similar or better surface coverage, the proposed method can outperform state-of-the-art NBV methods in terms of movement cost and inference time. Real-world experiments confirm that the proposed method can achieve faster object reconstruction than other methods.},
	number = {2},
	urldate = {2024-10-17},
	journal = {IEEE Robotics and Automation Letters},
	author = {Pan, Sicong and Hu, Hao and Wei, Hui},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Autonomous agents, Optimization, Path planning, Planning, Robots, Solid modeling, Surface reconstruction, Three-dimensional displays, computer vision for automation, data sets for robotic vision, deep learning for visual perception, motion and path planning},
	pages = {1463--1470},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} - {OctoMap}/octomap: {An} {Efficient} {Probabilistic} {3D} {Mapping} {Framework} {Based} on {Octrees}. {Contains} the main {OctoMap} library, the viewer octovis, and {dynamicEDT3D}.},
	shorttitle = {{GitHub} - {OctoMap}/octomap},
	url = {https://github.com/OctoMap/octomap},
	abstract = {An Efficient Probabilistic 3D Mapping Framework Based on Octrees. Contains the main OctoMap library, the viewer octovis, and dynamicEDT3D. - OctoMap/octomap},
	language = {en},
	urldate = {2024-10-17},
	journal = {GitHub},
}

@misc{noauthor_github_nodate-1,
	title = {{GitHub} - psc0628/{SCVP}-{Simulation}},
	url = {https://github.com/psc0628/SCVP-Simulation},
	abstract = {Contribute to psc0628/SCVP-Simulation development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-10-17},
	journal = {GitHub},
}

@misc{noauthor_github_nodate-2,
	title = {{GitHub} - irvingvasquez/nbv-net: {NBV}-{Net}: {A} {3D} {Convolutional} {Neural} {Network} for {Predicting} the {Next}-{Best}-{View}},
	shorttitle = {{GitHub} - irvingvasquez/nbv-net},
	url = {https://github.com/irvingvasquez/nbv-net},
	abstract = {NBV-Net: A 3D Convolutional Neural Network for Predicting the Next-Best-View - irvingvasquez/nbv-net},
	language = {en},
	urldate = {2024-10-17},
	journal = {GitHub},
}

@misc{noauthor_github_nodate-3,
	title = {{GitHub} - irvingvasquez/nbv\_regression: {Research} project for next-best-view regression based on supervised learning},
	shorttitle = {{GitHub} - irvingvasquez/nbv\_regression},
	url = {https://github.com/irvingvasquez/nbv_regression},
	abstract = {Research project for next-best-view regression based on supervised learning - irvingvasquez/nbv\_regression},
	language = {en},
	urldate = {2024-10-17},
	journal = {GitHub},
}

@misc{noauthor_nvidia_nodate,
	title = {{NVIDIA} {Jetson} {AGX} {Orin}},
	url = {https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/},
	abstract = {Next-level AI performance for next-gen robotics.},
	language = {en-us},
	urldate = {2024-10-14},
	journal = {NVIDIA},
}

@misc{noauthor_ros_nodate,
	title = {{ROS} 2 {Documentation} — {ROS} 2 {Documentation}: {Humble} documentation},
	url = {https://docs.ros.org/en/humble/index.html},
	urldate = {2024-10-09},
}

@misc{noauthor_gazebo_nodate,
	title = {Gazebo},
	url = {https://gazebosim.org/home},
	urldate = {2024-10-09},
}

@inproceedings{zeng_pc-nbv_2020,
	title = {{PC}-{NBV}: {A} {Point} {Cloud} {Based} {Deep} {Network} for {Efficient} {Next} {Best} {View} {Planning}},
	shorttitle = {{PC}-{NBV}},
	url = {https://ieeexplore.ieee.org/document/9340916/?arnumber=9340916},
	doi = {10.1109/IROS45743.2020.9340916},
	abstract = {The Next Best View (NBV) problem is important in the active robotic reconstruction. It enables the robot system to perform scanning actions in a reasonable view sequence, and fulfil the reconstruction task in an effective way. Previous works mainly follow the volumetric methods, which convert the point cloud information collected by sensors into a voxel representation space and evaluate candidate views through ray casting simulations to pick the NBV. However, the process of volumetric data transformation and ray casting is often time-consuming. To address this issue, in this paper, we propose a point cloud based deep neural network called PC-NBV to achieve efficient view planning without these computationally expensive operations. The PC-NBV network takes the raw point cloud data and current view selection states as input, and then directly predicts the information gain of all candidate views. By avoiding costly data transformation and ray casting, and utilizing powerful neural network to learn structure priors from point cloud, our method can achieve efficient and effective NBV planning. Experiments on multiple datasets show the proposed method outperforms state-of-the-art NBV methods, giving better views for robot system with much less inference time. Furthermore, we demonstrate the robustness of our method against noise and the ability to extend to multi-view system, making it more applicable for various scenarios.},
	urldate = {2024-10-08},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Zeng, Rui and Zhao, Wang and Liu, Yong-Jin},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Casting, Neural networks, Planning, Sensor systems, Task analysis, Three-dimensional displays, Trajectory},
	pages = {7050--7057},
}

@article{kohler_few-shot_2024,
	title = {Few-{Shot} {Object} {Detection}: {A} {Comprehensive} {Survey}},
	volume = {35},
	issn = {2162-2388},
	shorttitle = {Few-{Shot} {Object} {Detection}},
	url = {https://ieeexplore.ieee.org/document/10103630/?arnumber=10103630},
	doi = {10.1109/TNNLS.2023.3265051},
	abstract = {Humans are able to learn to recognize new objects even from a few examples. In contrast, training deep-learning-based object detectors requires huge amounts of annotated data. To avoid the need to acquire and annotate these huge amounts of data, few-shot object detection (FSOD) aims to learn from few object instances of new categories in the target domain. In this survey, we provide an overview of the state of the art in FSOD. We categorize approaches according to their training scheme and architectural layout. For each type of approach, we describe the general realization as well as concepts to improve the performance on novel categories. Whenever appropriate, we give short takeaways regarding these concepts in order to highlight the best ideas. Eventually, we introduce commonly used datasets and their evaluation protocols and analyze the reported benchmark results. As a result, we emphasize common challenges in evaluation and identify the most promising current trends in this emerging field of FSOD.},
	number = {9},
	urldate = {2024-10-08},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Köhler, Mona and Eisenbach, Markus and Gross, Horst-Michael},
	month = sep,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Benchmark testing, Detectors, Feature extraction, Few-shot learning, Object detection, Task analysis, Training, Transfer learning, meta learning, object detection, survey, transfer learning},
	pages = {11958--11978},
}

@article{tsuru_online_2021,
	title = {Online {Object} {Searching} by a {Humanoid} {Robot} in an {Unknown} {Environment}},
	volume = {6},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/9361266/?arnumber=9361266},
	doi = {10.1109/LRA.2021.3061383},
	abstract = {This letter proposes a framework for an autonomous humanoid robot, aimed at searching for a target object in an unknown environment using 3D-simultaneous localization and mapping (SLAM). The robot determines, while walking, the next viewpoint from an environment map and aggregated object recognition results, and automatically finds and grasps the target object. Whereas most robot exploration studies require a static map, hints regarding object position, area size limitations, or offline viewpoint planning time for each observation, our system can globally find an occluded object in an unknown environment, based only on the 3D target model. The biggest novelty of this research is that this framework always runs its viewpoint planner in background and immediately updates the destination if the camera gets environment/object information. To follow that goal change quickly, the humanoid robot re-plans its footstep trajectory without stopping, using foot landing estimation based on 3D-SLAM's localization. Notably, our robot can predict an unobserved area, and actively reveal it while avoiding obstacles. We validated the efficacy of this method through real experiments with an “HRP2-KAI” in several environments, and achieved fully automated searching and grasping.},
	number = {2},
	urldate = {2024-10-08},
	journal = {IEEE Robotics and Automation Letters},
	author = {Tsuru, Masato and Escande, Adrien and Tanguy, Arnaud and Chappellet, Kevin and Harad, Kensuke},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Humanoid robot systems, Humanoid robots, Legged locomotion, Planning, Robots, SLAM, Search problems, Three-dimensional displays, Two dimensional displays, perception-action coupling, vision-based navigation},
	pages = {2862--2869},
}

@article{naazare_online_2022,
	title = {Online {Next}-{Best}-{View} {Planner} for {3D}-{Exploration} and {Inspection} {With} a {Mobile} {Manipulator} {Robot}},
	volume = {7},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/9695293/?arnumber=9695293},
	doi = {10.1109/LRA.2022.3146558},
	abstract = {Robotic systems performing end-user oriented autonomous exploration can be deployed in different scenarios which not only require mapping but also simultaneous inspection of regions of interest for the end-user. In this work, we propose a novel Next-Best-View (NBV) planner which can perform full exploration and user-oriented exploration with inspection of the regions of interest using a mobile manipulator robot. We address the exploration-inspection problem as an instance of Multi-Objective Optimization (MOO) and propose a weighted-sum-based information gain function for computing NBVs for the RGB-D camera mounted on the arm. For both types of exploration tasks, we compare our approach with an existing state-of-the-art exploration method as the baseline and demonstrate our improvements in terms of total volume mapped and lower computational requirements. The real experiments with a mobile manipulator robot demonstrate the practicability and effectiveness of our approach outdoors.},
	number = {2},
	urldate = {2024-10-08},
	journal = {IEEE Robotics and Automation Letters},
	author = {Naazare, Menaka and Rosas, Francisco Garcia and Schulz, Dirk},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Contamination, Field robots, Inspection, Manipulators, Planning, Pollution measurement, Robots, environment monitoring and management, motion and path planning, reactive and sensor-based planning, robotics in hazardous fields},
	pages = {3779--3786},
}

@article{li_object-aware_2023,
	title = {Object-{Aware} {View} {Planning} for {Autonomous} 3-{D} {Model} {Reconstruction} of {Buildings} {Using} a {Mobile} {Robot}},
	volume = {72},
	issn = {1557-9662},
	url = {https://ieeexplore.ieee.org/document/10132570/?arnumber=10132570},
	doi = {10.1109/TIM.2023.3279424},
	abstract = {Active mapping ranks among the most critical applications of autonomous mobile robots. In this article, we focus on 3-D model reconstruction of buildings using an outdoor ground robot equipped with a solid-state light detection and ranging (LiDAR) and a gimbal. A view-planning approach is proposed to generate and evaluate view sequences with reference to the characteristics of solid-state LiDAR and the robot configuration. To reconstruct the specific buildings while ignoring unrelated objects in the environment, we introduce object awareness into the map representation, and frontier- and sampling-based strategies are combined to plan the view sequences for the effective observation of a target building. Instead of greedily maximizing information gain, frontier clusters indicating the mapped object’s incomplete surfaces are used to evaluate the sampled views and sampling the minimum view set generates view sequences able to efficiently cover the clusters. Moreover, using object awareness to partition the area of sampled views around the building facilitates a divide-and-conquer strategy that overcomes local characteristics and guides a complete reconstruction. Simulation and real-world experiments revealed that our approach allows the thorough and efficient active reconstruction of various buildings and even outperforms the state-of-the-art approaches.},
	urldate = {2024-10-08},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Li, Yuxiang and Wang, Jiancheng and Chen, Haoyao and Jiang, Xin and Liu, Yunhui},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Instrumentation and Measurement},
	keywords = {3-D model reconstruction, Buildings, Laser radar, Planning, Robot kinematics, Robots, Solid modeling, Three-dimensional displays, active mapping, object awareness, view planning},
	pages = {1--15},
}

@article{placed_survey_2023,
	title = {A {Survey} on {Active} {Simultaneous} {Localization} and {Mapping}: {State} of the {Art} and {New} {Frontiers}},
	volume = {39},
	issn = {1941-0468},
	shorttitle = {A {Survey} on {Active} {Simultaneous} {Localization} and {Mapping}},
	url = {https://ieeexplore.ieee.org/document/10075065/?arnumber=10075065},
	doi = {10.1109/TRO.2023.3248510},
	abstract = {Active simultaneous localization and mapping (SLAM) is the problem of planning and controlling the motion of a robot to build the most accurate and complete model of the surrounding environment. Since the first foundational work in active perception appeared, more than three decades ago, this field has received increasing attention across different scientific communities. This has brought about many different approaches and formulations, and makes a review of the current trends necessary and extremely valuable for both new and experienced researchers. In this article, we survey the state of the art in active SLAM and take an in-depth look at the open challenges that still require attention to meet the needs of modern applications. After providing a historical perspective, we present a unified problem formulation and review the well-established modular solution scheme, which decouples the problem into three stages that identify, select, and execute potential navigation actions. We then analyze alternative approaches, including belief-space planning and deep reinforcement learning techniques, and review related work on multirobot coordination. This article concludes with a discussion of new research directions, addressing reproducible research, active spatial perception, and practical applications, among other topics.},
	number = {3},
	urldate = {2024-10-08},
	journal = {IEEE Transactions on Robotics},
	author = {Placed, Julio A. and Strader, Jared and Carrillo, Henry and Atanasov, Nikolay and Indelman, Vadim and Carlone, Luca and Castellanos, José A.},
	month = jun,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Active perception, Location awareness, Navigation, Planning, Robot kinematics, Robots, Simultaneous localization and mapping, Uncertainty, active simultaneous localization and mapping (SLAM), autonomous robotic exploration, belief-space planning (BSP), deep reinforcement learning (DRL), next best view, optimality criteria},
	pages = {1686--1705},
}

@article{antonelli_few-shot_2022,
	title = {Few-{Shot} {Object} {Detection}: {A} {Survey}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Few-{Shot} {Object} {Detection}},
	url = {https://dl.acm.org/doi/10.1145/3519022},
	doi = {10.1145/3519022},
	abstract = {Deep learning approaches have recently raised the bar in many fields, from Natural Language Processing to Computer Vision, by leveraging large amounts of data. However, they could fail when the retrieved information is not enough to fit the vast number of parameters, frequently resulting in overfitting and therefore in poor generalizability. Few-Shot Learning aims at designing models that can effectively operate in a scarce data regime, yielding learning strategies that only need few supervised examples to be trained. These procedures are of both practical and theoretical importance, as they are crucial for many real-life scenarios in which data is either costly or even impossible to retrieve. Moreover, they bridge the distance between current data-hungry models and human-like generalization capability. Computer vision offers various tasks that can be few-shot inherent, such as person re-identification. This survey, which to the best of our knowledge is the first tackling this problem, is focused on Few-Shot Object Detection, which has received far less attention compared to Few-Shot Classification due to the intrinsic challenge level. In this regard, this review presents an extensive description of the approaches that have been tested in the current literature, discussing their pros and cons, and classifying them according to a rigorous taxonomy.},
	language = {en},
	number = {11s},
	urldate = {2024-10-08},
	journal = {ACM Computing Surveys},
	author = {Antonelli, Simone and Avola, Danilo and Cinque, Luigi and Crisostomi, Donato and Foresti, Gian Luca and Galasso, Fabio and Marini, Marco Raoul and Mecca, Alessio and Pannone, Daniele},
	month = jan,
	year = {2022},
	pages = {1--37},
}

@article{ren_faster_2017,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {39},
	issn = {1939-3539},
	shorttitle = {Faster {R}-{CNN}},
	url = {https://ieeexplore.ieee.org/document/7485869/?arnumber=7485869},
	doi = {10.1109/TPAMI.2016.2577031},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	number = {6},
	urldate = {2024-10-08},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jun,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Convolutional codes, Detectors, Feature extraction, Object detection, Proposals, Search problems, Training, convolutional neural network, region proposal},
	pages = {1137--1149},
}

@article{kim_autonomous_2022,
	title = {Autonomous {Exploration} in a {Cluttered} {Environment} for a {Mobile} {Robot} {With} {2D}-{Map} {Segmentation} and {Object} {Detection}},
	volume = {7},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/9765335/?arnumber=9765335},
	doi = {10.1109/LRA.2022.3171069},
	abstract = {Frontier-based exploration is widely adopted for exploring an unknown region. The conventional frontier-based exploration for a mobile robot may collide with three-dimensional (3D) obstacles or can suffer from a slower exploration time because the robot may move to another place before completely exploring the current area. To solve this problem, in this letter, we propose a new exploration algorithm by considering a path traveled by a mobile robot and segmenting a two-dimensional (2D) map. The segmented 2D map is generated in real-time by using the position of the robot and the location of the detected frontiers. To apply our algorithm to the actual experiment, we develop an object detection-based exploration algorithm that can remarkably reduce the probability of collision with 3D obstacles. To verify the effectiveness of our proposed algorithm, we perform simulations (Gazebo) and experiments (in the real world) to compare the conventional approach and our algorithm in a cluttered environment. The simulation and experiment results show that our algorithm can satisfactorily shorten the exploration path and time.},
	number = {3},
	urldate = {2024-10-08},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kim, Hyungseok and Kim, Hyeongjin and Lee, Seonil and Lee, Hyeonbeom},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Classification algorithms, Collision avoidance, Mobile robots, Planning under uncertainty, Prediction algorithms, Robot sensing systems, Three-dimensional displays, object detection, search and rescue robots, segmentation and categorization},
	pages = {6343--6350},
}

@inproceedings{vasquez-gomez_vpl_2020,
	title = {{VPL}: {A} {View} {Planning} {Library} for {Automated} {3D} {Reconstruction}},
	shorttitle = {{VPL}},
	url = {https://ieeexplore.ieee.org/document/9359430/?arnumber=9359430},
	abstract = {During the last decade, automated three-dimensional (3D) object reconstruction and inspection have become widely used in the same way that 3D sensors and positioning systems have become affordable. For example, nowadays drones or underwater vehicles are capable of building 3D models of complex structures. To achieve an autonomous reconstruction it is essential to determine each pose of the sensor, from where an observation is made. In the literature, this task is called view planning. To develop a custom application or propose a new view planning algorithm requires several software tools that are usually reimplemented each time; this causes an effort duplication and is one of the reasons why view planning research papers usually do not present comparisons. We present VPL, a software framework to develop view planning algorithms and compare their performance versus other approaches. The library is written in C++ and it is released under open source BSD license.},
	urldate = {2024-10-08},
	author = {Vasquez-Gomez, J. Irving},
	month = nov,
	year = {2020},
	keywords = {3d reconstruction, Libraries, Planning, Sensors, Software algorithms, Task analysis, Three-dimensional displays, Underwater vehicles, next best view, view planning},
}

@inproceedings{vasquez-gomez_view_2014,
	title = {View planning for {3D} object reconstruction with a mobile manipulator robot},
	url = {https://ieeexplore.ieee.org/document/6943158/?arnumber=6943158},
	doi = {10.1109/IROS.2014.6943158},
	abstract = {The task addressed in this paper is to plan iteratively a set views in order to reconstruct an object using a mobile manipulator robot with an “eye-in-hand” sensor. The proposed method plans views directly in the configuration space avoiding the need of inverse kinematics. It is based on a fast evaluation and rejection of a set of candidate configurations. The main contributions are: a utility function to rank the views and an evaluation strategy implemented as a series of filters. Given that the candidate views are configurations, motion planning is solved using a rapidly-exploring random tree. The system is experimentally evaluated in simulation, contrasting it with previous work. We also present experiments with a real mobile manipulator robot, demonstrating the effectiveness of our method.},
	urldate = {2024-10-08},
	booktitle = {2014 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Vasquez-Gomez, J. Irving and Sucar, L. Enrique and Murrieta-Cid, Rafael},
	month = sep,
	year = {2014},
	note = {ISSN: 2153-0866},
	keywords = {Collision avoidance, Manipulators, Mobile communication, Octrees, Ray tracing, Robot sensing systems},
	pages = {4227--4233},
}

@inproceedings{kriegel_next-best-scan_2012,
	title = {Next-best-scan planning for autonomous {3D} modeling},
	url = {https://ieeexplore.ieee.org/document/6385624/?arnumber=6385624},
	doi = {10.1109/IROS.2012.6385624},
	abstract = {We present a next-best-scan (NBS) planning approach for autonomous 3D modeling. The system successively completes a 3D model from complex shaped objects by iteratively selecting a NBS based on previously acquired data. For this purpose, new range data is accumulated in-the-loop into a 3D surface (streaming reconstruction) and new continuous scan paths along the estimated surface trend are generated. Further, the space around the object is explored using a probabilistic exploration approach that considers sensor uncertainty. This allows for collision free path planning in order to completely scan unknown objects. For each scan path, the expected information gain is determined and the best path is selected as NBS. The presented NBS approach is tested with a laser striper system, attached to an industrial robot. The results are compared to state-of-the-art next-best-view methods. Our results show promising performance with respect to completeness, quality and scan time.},
	urldate = {2024-10-08},
	booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Kriegel, Simon and Rink, Christian and Bodenmüller, Tim and Narr, Alexander and Suppa, Michael and Hirzinger, Gerd},
	month = oct,
	year = {2012},
	note = {ISSN: 2153-0866},
	keywords = {Collision avoidance, NIST, Planning, Robot sensing systems, Solid modeling, Three dimensional displays},
	pages = {2850--2856},
}

@article{faulhammer_autonomous_2017,
	title = {Autonomous {Learning} of {Object} {Models} on a {Mobile} {Robot}},
	volume = {2},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/7393491/?arnumber=7393491},
	doi = {10.1109/LRA.2016.2522086},
	abstract = {In this article, we present and evaluate a system, which allows a mobile robot to autonomously detect, model, and re-recognize objects in everyday environments. While other systems have demonstrated one of these elements, to our knowledge, we present the first system, which is capable of doing all of these things, all without human interaction, in normal indoor scenes. Our system detects objects to learn by modeling the static part of the environment and extracting dynamic elements. It then creates and executes a view plan around a dynamic element to gather additional views for learning. Finally, these views are fused to create an object model. The performance of the system is evaluated on publicly available datasets as well as on data collected by the robot in both controlled and uncontrolled scenarios.},
	number = {1},
	urldate = {2024-10-08},
	journal = {IEEE Robotics and Automation Letters},
	author = {Fäulhammer, Thomas and Ambruş, Rareş and Burbridge, Chris and Zillich, Michael and Folkesson, John and Hawes, Nick and Jensfelt, Patric and Vincze, Markus},
	month = jan,
	year = {2017},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Autonomous Agents, Cameras, Heuristic algorithms, Mobile robots, Motion and Path Planning, Object detection, RGB-D Perception, Robot vision systems, Solid modeling, Three-dimensional displays, Visual Learning, categorization, segmentation},
	pages = {26--33},
}

@inproceedings{krainin_autonomous_2011,
	title = {Autonomous generation of complete {3D} object models using next best view manipulation planning},
	url = {https://ieeexplore.ieee.org/document/5980429/?arnumber=5980429},
	doi = {10.1109/ICRA.2011.5980429},
	abstract = {Recognizing and manipulating objects is an important task for mobile robots performing useful services in everyday environments. In this paper, we develop a system that enables a robot to grasp an object and to move it in front of its depth camera so as to build a 3D surface model of the object. We derive an information gain based variant of the next best view algorithm in order to determine how the manipulator should move the object in front of the camera. By considering occlusions caused by the robot manipulator, our technique also determines when and how the robot should re-grasp the object in order to build a complete model.},
	urldate = {2024-10-08},
	booktitle = {2011 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Krainin, Michael and Curless, Brian and Fox, Dieter},
	month = may,
	year = {2011},
	note = {ISSN: 1050-4729},
	keywords = {Cameras, Manipulators, Robot vision systems, Surface reconstruction, Uncertainty},
	pages = {5031--5037},
}

@inproceedings{foissotte_next-best-view_2008,
	title = {A next-best-view algorithm for autonomous {3D} object modeling by a humanoid robot},
	url = {https://ieeexplore.ieee.org/document/4756001/?arnumber=4756001},
	doi = {10.1109/ICHR.2008.4756001},
	abstract = {We present our investigation to make humanoids build autonomously geometric models of unknown objects. Although good methods have been proposed for the specific problem of the next-best-view during the modeling and the recognition process; our approach is different and takes into account humanoid specificities in terms of embedded vision sensor and redundant motion capabilities. The problem to select the best next view of interest at each modeling step is formulated as an optimization problem where the whole robot posture needs to be defined jointly with the robot cameraspsila position and orientation. To achieve this, we propose a differentiable formula that expresses the amount of unknown data visible from a specific viewpoint, given only knowledge acquired in previous steps. In addition, a specific stability constraint is introduced to allow the robot to reach a configuration where its feet can be moved away from their initial position.},
	urldate = {2024-10-08},
	booktitle = {Humanoids 2008 - 8th {IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Foissotte, T. and Stasse, O. and Escande, A. and Kheddar, A.},
	month = dec,
	year = {2008},
	note = {ISSN: 2164-0580},
	keywords = {Cameras, Humanoid robots, Humans, Object detection, Object recognition, Robot sensing systems, Robot vision systems, Robustness, Solid modeling, Stability},
	pages = {333--338},
}

@inproceedings{wong_next_1999,
	title = {Next best view system in a {3D} object modeling task},
	url = {https://ieeexplore.ieee.org/document/810066/?arnumber=810066},
	doi = {10.1109/CIRA.1999.810066},
	abstract = {Sensor placement for 3D modeling is a growing area of computer vision and robotics. The objective of a sensor placement system is to make task-directed decisions for optimal pose selection. We propose a next best view solution to the sensor placement problem. Our algorithm computes the next best view by optimizing an objective function that measures the quantity of unknown information in each of a group of potential viewpoints. The potential views are either placed uniformly around the object or are calculated from the surface normals of the occupancy grid model. To initiate the collection of new data, the optimal pose is selected from the objective function calculation. The model is incrementally updated from the information acquired in each new view. This process terminates when the number of recovered voxels ceases to increase, yielding the final model. We tested two different algorithms on 8 objects of various complexity, including objects with simple concave, simple hole, and complex hole self-occlusions.},
	urldate = {2024-10-08},
	booktitle = {Proceedings 1999 {IEEE} {International} {Symposium} on {Computational} {Intelligence} in {Robotics} and {Automation}. {CIRA}'99 ({Cat}. {No}.{99EX375})},
	author = {Wong, L.M. and Dumont, C. and Abidi, M.A.},
	month = nov,
	year = {1999},
	keywords = {Cameras, Computational modeling, Computer simulation, Image resolution, Optimization methods, Pixel, Shape, System software},
	pages = {306--311},
}

@inproceedings{connolly_determination_1985,
	title = {The determination of next best views},
	volume = {2},
	url = {https://ieeexplore.ieee.org/document/1087372/?arnumber=1087372},
	doi = {10.1109/ROBOT.1985.1087372},
	abstract = {There are situations in which one would like to know a good sequence of range-image views for obtaining a complete model of a scene. This paper describes two algorithms which use partial octree models to determine the "best" next view to take.},
	urldate = {2024-10-08},
	booktitle = {1985 {IEEE} {International} {Conference} on {Robotics} and {Automation} {Proceedings}},
	author = {Connolly, C.},
	month = mar,
	year = {1985},
	keywords = {Area measurement, Layout, Sampling methods, Turbines},
	pages = {432--435},
}

@misc{noauthor_spot_nodate,
	title = {Spot},
	url = {https://bostondynamics.com/products/spot/},
	abstract = {Spot is changing how organizations monitor and operate their sites. Improve team safety and efficiency with agile mobile robot solutions from Boston Dynamics.},
	language = {en-US},
	urldate = {2024-10-08},
	journal = {Boston Dynamics},
}

@article{vasquez-gomez_tree-based_2018,
	title = {Tree-based search of the next best view/state for three-dimensional object reconstruction},
	volume = {15},
	issn = {1729-8806},
	url = {https://doi.org/10.1177/1729881418754575},
	doi = {10.1177/1729881418754575},
	abstract = {Three-dimensional models from real objects have many applications in robotics. To automatically build a three-dimensional model from an object, it is essential to determine where to place the range sensor in order to completely observe the object. However, the view (position and orientation) of the sensor is not sufficient, given that its corresponding robot state needs to be calculated. Additionally, a collision-free trajectory to reach that state is required. In this article, we directly find the state of the robot whose corresponding sensor view observes the object. This method does not require to calculate the inverse kinematics of the robot. Unlike previous approaches, the proposed method guides the search with a tree structure based on a rapidly exploring random tree overcoming previous sampling techniques. In addition, we propose an information metric that improves the reconstruction performance of previous information metrics.},
	language = {en},
	number = {1},
	urldate = {2024-10-02},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Vasquez-Gomez, J Irving and Sucar, L Enrique and Murrieta-Cid, Rafael and Herrera-Lozada, Juan-Carlos},
	month = jan,
	year = {2018},
	note = {Publisher: SAGE Publications},
	pages = {1729881418754575},
}

@inproceedings{han_double_2022,
	title = {A {Double} {Branch} {Next}-{Best}-{View} {Network} and {Novel} {Robot} {System} for {Active} {Object} {Reconstruction}},
	url = {https://ieeexplore.ieee.org/document/9811769/?arnumber=9811769},
	doi = {10.1109/ICRA46639.2022.9811769},
	abstract = {Next best view (NBV) is a technology that finds the best view sequence for sensor to perform scanning based on partial information, which is the core part for robot active reconstruction. Traditional works are mostly based on the evaluation of candidate views through time-consuming volu-metric transformation and ray casting, which heavily limits the applications of NBV. Recent deep learning based NBV methods aim to approximately learn the evaluation function by large-scale training, and improve both the effectiveness and efficiency of NBV. However, these methods force the network to regress the exact groundtruth value of each candidate view, which is much harder than simply ranking all the candidate views. Besides, most previous NBV works assume perfect sensing and perform in simulation environments, lacking real application abilities. In this paper, we propose a novel double branch NBV network, DB-NBV, to utilize the ranking process together with the evaluation process. We further design a real NBV robot and a pipeline to conduct real active reconstruction. Experiments on both simulation and real robot show that our method achieves the best performance and can be applied to real application with high accuracy and speed.},
	urldate = {2024-10-02},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Han, Yiheng and Zhan, Irvin Haozhe and Zhao, Wang and Liu, Yong-Jin},
	month = may,
	year = {2022},
	keywords = {Automation, Casting, Deep learning, Force, Pipelines, Robot sensing systems, Training},
	pages = {7306--7312},
}

@article{vasquez-gomez_next-best-view_2021,
	title = {Next-best-view regression using a {3D} convolutional neural network},
	volume = {32},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-020-01166-2},
	doi = {10.1007/s00138-020-01166-2},
	abstract = {Automated three-dimensional (3D) object reconstruction is the task of building a geometric representation of a physical object by means of sensing its surface. Even though new single-view reconstruction techniques can predict the surface, they lead to incomplete models, specially, for non-commons objects such as antique objects or art sculptures. Therefore, to achieve the task’s goals, it is essential to automatically determine the locations where the sensor will be placed so that the surface will be completely observed. This problem is known as the next-best-view problem. In this paper, we propose a data-driven approach to address the problem. The proposed approach trains a 3D convolutional neural network (3D CNN) with previous reconstructions in order to regress the position of the next-best-view. To the best of our knowledge, this is one of the first works that directly infers the next-best-view in a continuous space using a data-driven approach for the 3D object reconstruction task. We have validated the proposed approach making use of two groups of experiments. In the first group, several variants of the proposed architecture are analyzed. Predicted next-best-views were observed to be closely positioned to the ground truth. In the second group of experiments, the proposed approach is requested to reconstruct several unseen objects, namely, objects not considered by the 3D CNN during training nor validation. Coverage percentages of up to 90 \% were observed. With respect to current state-of-the-art methods, the proposed approach improves the performance of previous next-best-view classification approaches and it is quite fast in running time (3 frames per second), given that it does not compute the expensive ray tracing required by previous information metrics.},
	language = {en},
	number = {2},
	urldate = {2024-10-02},
	journal = {Machine Vision and Applications},
	author = {Vasquez-Gomez, J. Irving and Troncoso, David and Becerra, Israel and Sucar, Enrique and Murrieta-Cid, Rafael},
	month = jan,
	year = {2021},
	keywords = {3D modeling, Artificial Intelligence, Deep learning, Next-best-view, Object reconstruction, Range sensing},
	pages = {42},
}

@article{vasquez-gomez_viewstate_2017,
	title = {View/state planning for three-dimensional object reconstruction under uncertainty},
	volume = {41},
	issn = {1573-7527},
	url = {https://doi.org/10.1007/s10514-015-9531-3},
	doi = {10.1007/s10514-015-9531-3},
	abstract = {We propose a holistic approach for three-dimensional (3D) object reconstruction with a mobile manipulator robot with an eye-in-hand sensor; considering the plan to reach the desired view/state, and the uncertainty in both observations and controls. This is one of the first methods that determines the next best view/state in the state space, following a methodology in which a set of candidate views/states is directly generated in the state space, and later only a subset of these views is kept by filtering the original set. It also determines the controls that yield a collision free trajectory to reach a state using rapidly-exploring random trees. To decrease the processing time we propose an efficient evaluation strategy based on filters, and a 3D visibility calculation with hierarchical ray tracing. The next best view/state is selected based on the expected utility, generating samples in the control space based on an error distribution according to the dynamics of the robot. This makes the method robust to positioning error, significantly reducing the collision rate and increasing the coverage, as shown in the experiments. Several experiments in simulation and with a real mobile manipulator robot with 8 degrees of freedom show that the proposed method provides an effective and fast method for a mobile manipulator to build 3D models of unknown objects. To our knowledge, this is one of the first works that demonstrates the reconstruction of complex objects with a real mobile manipulator considering uncertainty in the controls.},
	language = {en},
	number = {1},
	urldate = {2024-10-02},
	journal = {Autonomous Robots},
	author = {Vasquez-Gomez, J. Irving and Sucar, L. Enrique and Murrieta-Cid, Rafael},
	month = jan,
	year = {2017},
	keywords = {Artificial Intelligence, Motion planning, Next best view, Object reconstruction, Uncertainty},
	pages = {89--109},
}

@inproceedings{cui_multi-sensor_2019,
	title = {A {Multi}-{Sensor} {Next}-{Best}-{View} {Framework} for {Geometric} {Model}-{Based} {Robotics} {Applications}},
	url = {https://ieeexplore.ieee.org/abstract/document/8794423?casa_token=MBOle6YaSowAAAAA:DFebpwBzG0EwBHX9ud08M48tt1hDgIneRrrgq9svdryy-_IgM0fm4JvPaw1gAQ4KFOm9IQWEVw},
	doi = {10.1109/ICRA.2019.8794423},
	abstract = {Geometric models are crucial for many robotics applications. Current robotic 3D reconstruction systems only focus on specific reconstruction goals which make them hard to adapt to different tasks. In this paper we present a next-best-view framework which allows robots to construct a geometric model incrementally through consecutive sensing actions. Instead of limiting the type and total number of sensors, in each sensing step we evaluate actions from all available sensors and pick the best to execute. Our framework is more comprehensive since the model building process can be designed to best accomplish different tasks. The system has been demonstrated in two experiments on 3D reconstruction and weld seam inspection, yielding promising results.},
	urldate = {2024-10-02},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Cui, Jinda and Wen, John T. and Trinkle, Jeff},
	month = may,
	year = {2019},
	note = {ISSN: 2577-087X},
	keywords = {Cameras, Modeling, MultiSensor, Robot sensing systems, Task analysis, Three-dimensional displays},
	pages = {8769--8775},
}

@article{wu_quality-driven_2014,
	title = {Quality-driven {Poisson}-guided {Autoscanning}},
	url = {http://kops.uni-konstanz.de/handle/123456789/29847},
	abstract = {We present a quality-driven, Poisson-guided autonomous scanning method. Unlike previous scan planning techniques, we do not aim to minimize the number of scans needed to cover the object’s surface, but rather to ensure the high quality scanning of the model. This goal is achieved by placing the scanner at strategically selected{\textless}br /{\textgreater}Next-Best-Views (NBVs) to ensure progressively capturing the geometric details of the object, until both completeness and high fidelity are reached. The technique is based on the analysis of a Poisson field and its geometric relation with an input scan. We{\textless}br /{\textgreater}generate a confidence map that reflects the quality/fidelity of the estimated Poisson iso-surface. The confidence map guides the generation of a viewing vector field, which is then used for computing a set of NBVs. We applied the algorithm on two different robotic platforms, a PR2 mobile robot and a one-arm industry robot. We demonstrated the advantages of our method through a number of autonomous high quality scannings of complex physical objects, as well as performance comparisons against state-of-the-art methods.},
	language = {eng},
	urldate = {2024-10-02},
	author = {Wu, Shihao and Sun, Wei and Long, Pinxin and Huang, Hui and Cohen-Or, Daniel and Gong, Minglun and Deussen, Oliver and Chen, Baoquan},
	year = {2014},
}

@article{zeng_view_2020,
	title = {View planning in robot active vision: {A} survey of systems, algorithms, and applications},
	volume = {6},
	issn = {2096-0662},
	shorttitle = {View planning in robot active vision},
	url = {https://doi.org/10.1007/s41095-020-0179-3},
	doi = {10.1007/s41095-020-0179-3},
	abstract = {Rapid development of artificial intelligence motivates researchers to expand the capabilities of intelligent and autonomous robots. In many robotic applications, robots are required to make planning decisions based on perceptual information to achieve diverse goals in an efficient and effective way. The planning problem has been investigated in active robot vision, in which a robot analyzes its environment and its own state in order to move sensors to obtain more useful information under certain constraints. View planning, which aims to find the best view sequence for a sensor, is one of the most challenging issues in active robot vision. The quality and efficiency of view planning are critical for many robot systems and are influenced by the nature of their tasks, hardware conditions, scanning states, and planning strategies. In this paper, we first summarize some basic concepts of active robot vision, and then review representative work on systems, algorithms and applications from four perspectives: object reconstruction, scene reconstruction, object recognition, and pose estimation. Finally, some potential directions are outlined for future work.},
	language = {en},
	number = {3},
	urldate = {2024-09-17},
	journal = {Computational Visual Media},
	author = {Zeng, Rui and Wen, Yuhui and Zhao, Wang and Liu, Yong-Jin},
	month = sep,
	year = {2020},
	keywords = {Artificial Intelligence, active vision, next-best view, robotic, sensor planning, state-of-the-art, view planning},
	pages = {225--245},
}

@article{mendoza_supervised_2020,
	title = {Supervised learning of the next-best-view for 3d object reconstruction},
	volume = {133},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865518305531},
	doi = {10.1016/j.patrec.2020.02.024},
	abstract = {Motivated by the advances in 3D sensing technology and the spreading of low-cost robotic platforms, 3D object reconstruction has become a common task in many areas. Nevertheless, the selection of the optimal sensor pose that maximizes the reconstructed surface is a problem that remains open. It is known in the literature as the next-best-view planning problem. In this paper, we propose a novel next-best-view planning scheme based on supervised deep learning. The scheme contains an algorithm for automatic generation of datasets and an original three-dimensional convolutional neural network (3D-CNN) used to learn the next-best-view. Unlike previous work where the problem is addressed as a search, the trained 3D-CNN directly predicts the sensor pose. We present an experimental comparison of the proposed architecture against two alternative networks; we also compare it with state-of-the-art next-best-view methods in the reconstruction of several unknown objects. Our method is faster and reaches high coverage.},
	urldate = {2024-09-18},
	journal = {Pattern Recognition Letters},
	author = {Mendoza, Miguel and Vasquez-Gomez, J. Irving and Taud, Hind and Sucar, L. Enrique and Reta, Carolina},
	month = may,
	year = {2020},
	keywords = {3D reconstruction, 3D-CNN, Next-best-view, state-of-the-art},
	pages = {224--231},
}

@article{wu_plant_2019,
	title = {Plant {Phenotyping} by {Deep}-{Learning}-{Based} {Planner} for {Multi}-{Robots}},
	volume = {4},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/8743431?casa_token=2GSxc4Bf7X0AAAAA:lXea1atCVol5IeSLS1HLodxYKc-CrkQssfUZQc9bfvaV2rmRiiIOjmtWQvzC19biOXKVh_6ing},
	doi = {10.1109/LRA.2019.2924125},
	abstract = {Manual plant phenotyping is slow, error prone, and labor intensive. In this letter, we present an automated robotic system for fast, precise, and noninvasive measurements using a new deep-learning-based next-best view planning pipeline. Specifically, we first use a deep neural network to estimate a set of candidate voxels for the next scanning. Next, we cast rays from these voxels to determine the optimal viewpoints. We empirically evaluate our method in simulations and real-world robotic experiments with up to three robotic arms to demonstrate its efficiency and effectiveness. One advantage of our new pipeline is that it can be easily extended to a multi-robot system where multiple robots move simultaneously according to the planned motions. Our system significantly outperforms the single robot in flexibility and planning time. High-throughput phenotyping can be made practically.},
	number = {4},
	urldate = {2024-10-02},
	journal = {IEEE Robotics and Automation Letters},
	author = {Wu, Chenming and Zeng, Rui and Pan, Jia and Wang, Charlie C. L. and Liu, Yong-Jin},
	month = oct,
	year = {2019},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Agricultural automation, Manipulators, Multi-robot systems, Planning, Robot sensing systems, Three-dimensional displays, computer vision for automation, multi-robot systems},
	pages = {3113--3120},
}

@inproceedings{devrim_kaba_reinforcement_2017,
	title = {A {Reinforcement} {Learning} {Approach} to the {View} {Planning} {Problem}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Kaba_A_Reinforcement_Learning_CVPR_2017_paper.html},
	urldate = {2024-10-02},
	author = {Devrim Kaba, Mustafa and Gokhan Uzunbas, Mustafa and Nam Lim, Ser},
	year = {2017},
	pages = {6933--6941},
}

@article{arents_smart_2022,
	title = {Smart {Industrial} {Robot} {Control} {Trends}, {Challenges} and {Opportunities} within {Manufacturing}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/2/937},
	doi = {10.3390/app12020937},
	abstract = {Industrial robots and associated control methods are continuously developing. With the recent progress in the field of artificial intelligence, new perspectives in industrial robot control strategies have emerged, and prospects towards cognitive robots have arisen. AI-based robotic systems are strongly becoming one of the main areas of focus, as flexibility and deep understanding of complex manufacturing processes are becoming the key advantage to raise competitiveness. This review first expresses the significance of smart industrial robot control in manufacturing towards future factories by listing the needs, requirements and introducing the envisioned concept of smart industrial robots. Secondly, the current trends that are based on different learning strategies and methods are explored. Current computer-vision, deep reinforcement learning and imitation learning based robot control approaches and possible applications in manufacturing are investigated. Gaps, challenges, limitations and open issues are identified along the way.},
	language = {en},
	number = {2},
	urldate = {2024-09-20},
	journal = {Applied Sciences},
	author = {Arents, Janis and Greitans, Modris},
	month = jan,
	year = {2022},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, cognitive robotics, computer vision, future factories, imitation learning, reinforcement learning, simulation, smart industrial robots, smart manufacturing, synthetic data},
	pages = {937},
}

@article{maboudi_review_2023,
	title = {A {Review} on {Viewpoints} and {Path} {Planning} for {UAV}-{Based} 3-{D} {Reconstruction}},
	volume = {16},
	issn = {2151-1535},
	url = {https://ieeexplore.ieee.org/abstract/document/10124957},
	doi = {10.1109/JSTARS.2023.3276427},
	abstract = {Unmanned aerial vehicles (UAVs) are widely used platforms to carry data capturing sensors for various applications. The reason for this success can be found in many aspects: the high maneuverability of the UAVs, the capability of performing autonomous data acquisition, flying at different heights, and the possibility to reach almost any vantage point. The selection of appropriate viewpoints and planning the optimum trajectories of UAVs is an emerging topic that aims at increasing the automation, efficiency, and reliability of the data capturing process to achieve a dataset with desired quality. On the other hand, 3-D reconstruction using the data captured by UAVs is also attracting attention in research and industry. This article investigates a wide range of model-free and model-based algorithms for viewpoints and path planning for 3-D reconstruction of large-scale objects. It presents a bibliography of more than 200 references to cover different aspects of the topic. The analyzed approaches are limited to those that employ a single-UAV as a data capturing platform for outdoor 3-D reconstruction purposes. In addition to discussing the evaluation strategies, this article also highlights the innovations and limitations of the investigated approaches. It concludes with a critical analysis of the existing challenges and future research perspectives.},
	urldate = {2024-09-18},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Maboudi, Mehdi and Homaei, MohammadReza and Song, Soohwan and Malihi, Shirin and Saadatseresht, Mohammad and Gerke, Markus},
	year = {2023},
	note = {Conference Name: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	keywords = {3-D displays, Aircraft navigation path planning, Cameras, Image reconstruction, Planning, Robots, Sensors, Solid modeling, Three-dimensional displays, autonomous aerial vehicles, image reconstruction, motion planning, remotely guided vehicles, surface reconstruction, viewpoints planning},
	pages = {5026--5048},
}

@inproceedings{isler_information_2016,
	title = {An information gain formulation for active volumetric {3D} reconstruction},
	url = {https://ieeexplore.ieee.org/abstract/document/7487527},
	doi = {10.1109/ICRA.2016.7487527},
	abstract = {We consider the problem of next-best view selection for volumetric reconstruction of an object by a mobile robot equipped with a camera. Based on a probabilistic volumetric map that is built in real time, the robot can quantify the expected information gain from a set of discrete candidate views. We propose and evaluate several formulations to quantify this information gain for the volumetric reconstruction task, including visibility likelihood and the likelihood of seeing new parts of the object. These metrics are combined with the cost of robot movement in utility functions. The next best view is selected by optimizing these functions, aiming to maximize the likelihood of discovering new parts of the object. We evaluate the functions with simulated and real world experiments within a modular software system that is adaptable to other robotic platforms and reconstruction problems. We release our implementation open source.},
	urldate = {2024-09-18},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Isler, Stefan and Sabzevari, Reza and Delmerico, Jeffrey and Scaramuzza, Davide},
	month = may,
	year = {2016},
	keywords = {Cameras, Entropy, Image reconstruction, Probabilistic logic, Robot sensing systems, Three-dimensional displays},
	pages = {3477--3484},
}

@article{kriegel_efficient_2015,
	title = {Efficient next-best-scan planning for autonomous {3D} surface reconstruction of unknown objects},
	volume = {10},
	issn = {1861-8219},
	url = {https://doi.org/10.1007/s11554-013-0386-6},
	doi = {10.1007/s11554-013-0386-6},
	abstract = {This work focuses on autonomous surface reconstruction of small-scale objects with a robot and a 3D sensor. The aim is a high-quality surface model allowing for robotic applications such as grasping and manipulation. Our approach comprises the generation of next-best-scan (NBS) candidates and selection criteria, error minimization between scan patches and termination criteria. NBS candidates are iteratively determined by a boundary detection and surface trend estimation of the acquired model. To account for both a fast and high-quality model acquisition, that candidate is selected as NBS, which maximizes a utility function that integrates an exploration and a mesh-quality component. The modeling and scan planning methods are evaluated on an industrial robot with a high-precision laser striper system. While performing the new laser scan, data are integrated on-the-fly into both, a triangle mesh and a probabilistic voxel space. The efficiency of the system in fast acquisition of high-quality 3D surface models is proven with different cultural heritage, household and industrial objects.},
	language = {en},
	number = {4},
	urldate = {2024-09-18},
	journal = {Journal of Real-Time Image Processing},
	author = {Kriegel, Simon and Rink, Christian and Bodenmüller, Tim and Suppa, Michael},
	month = dec,
	year = {2015},
	keywords = {3D modeling, Active vision, Laser scanning, Next-best-view planning},
	pages = {611--631},
}

@article{delmerico_comparison_2018,
	title = {A comparison of volumetric information gain metrics for active {3D} object reconstruction},
	volume = {42},
	issn = {1573-7527},
	url = {https://doi.org/10.1007/s10514-017-9634-0},
	doi = {10.1007/s10514-017-9634-0},
	abstract = {In this paper, we investigate the following question: when performing next best view selection for volumetric 3D reconstruction of an object by a mobile robot equipped with a dense (camera-based) depth sensor, what formulation of information gain is best? To address this question, we propose several new ways to quantify the volumetric information (VI) contained in the voxels of a probabilistic volumetric map, and compare them to the state of the art with extensive simulated experiments. Our proposed formulations incorporate factors such as visibility likelihood and the likelihood of seeing new parts of the object. The results of our experiments allow us to draw some clear conclusions about the VI formulations that are most effective in different mobile-robot reconstruction scenarios. To the best of our knowledge, this is the first comparative survey of VI formulation performance for active 3D object reconstruction. Additionally, our modular software framework is adaptable to other robotic platforms and general reconstruction problems, and we release it open source for autonomous reconstruction tasks.},
	language = {en},
	number = {2},
	urldate = {2024-09-18},
	journal = {Autonomous Robots},
	author = {Delmerico, Jeffrey and Isler, Stefan and Sabzevari, Reza and Scaramuzza, Davide},
	month = feb,
	year = {2018},
	keywords = {3D reconstruction, Active vision, Artificial Intelligence, Important, Information gain},
	pages = {197--208},
}

@inproceedings{banta_best-next-view_1995,
	title = {Best-next-view algorithm for three-dimensional scene reconstruction using range images},
	volume = {2588},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/2588/0000/Best-next-view-algorithm-for-three-dimensional-scene-reconstruction-using/10.1117/12.222691.full},
	doi = {10.1117/12.222691},
	abstract = {The primary focus of the research detailed in this paper is to develop an intelligent sensing module capable of automatically determining the optimal next sensor position and orientation during scene reconstruction. To facilitate a solution to this problem, we have assembled a system for reconstructing a 3D model of an object or scene from a sequence of range images. Candidates for the best-next-view position are determined by detecting and measuring occlusions to the range camera's view in an image. Ultimately, the candidate which will reveal the greatest amount of unknown scene information is selected as the best-next-view position. Our algorithm uses ray tracing to determine how much new information a given sensor perspective will reveal. We have tested our algorithm successfully on several synthetic range data streams, and found the system's results to be consistent with an intuitive human search. The models recovered by our system from range data compared well with the ideal models. Essentially, we have proven that range information of physical objects can be employed to automatically reconstruct a satisfactory dynamic 3D computer model at a minimal computational expense. This has obvious implications in the contexts of robot navigation, manufacturing, and hazardous materials handling. The algorithm we developed takes advantage of no a priori information in finding the best-next-view position.},
	urldate = {2024-09-18},
	booktitle = {Intelligent {Robots} and {Computer} {Vision} {XIV}: {Algorithms}, {Techniques}, {Active} {Vision}, and {Materials} {Handling}},
	publisher = {SPIE},
	author = {Banta, J. E. and Zhien, Yu and Wang, X. Z. and Zhang, G. and Smith, M. T. and Abidi, Mongi A.},
	month = oct,
	year = {1995},
	pages = {418--429},
}

@inproceedings{gidaris_dynamic_2018,
	title = {Dynamic {Few}-{Shot} {Visual} {Learning} {Without} {Forgetting}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.html},
	urldate = {2024-09-13},
	author = {Gidaris, Spyros and Komodakis, Nikos},
	year = {2018},
	pages = {4367--4375},
}

@inproceedings{fan_few-shot_2020,
	title = {Few-{Shot} {Object} {Detection} {With} {Attention}-{RPN} and {Multi}-{Relation} {Detector}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Few-Shot_Object_Detection_With_Attention-RPN_and_Multi-Relation_Detector_CVPR_2020_paper.html},
	urldate = {2024-09-13},
	author = {Fan, Qi and Zhuo, Wei and Tang, Chi-Keung and Tai, Yu-Wing},
	year = {2020},
	pages = {4013--4022},
}

@inproceedings{perez-rua_incremental_2020,
	title = {Incremental {Few}-{Shot} {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Perez-Rua_Incremental_Few-Shot_Object_Detection_CVPR_2020_paper.html},
	urldate = {2024-09-13},
	author = {Perez-Rua, Juan-Manuel and Zhu, Xiatian and Hospedales, Timothy M. and Xiang, Tao},
	year = {2020},
	pages = {13846--13855},
}

@misc{wang_frustratingly_2020,
	title = {Frustratingly {Simple} {Few}-{Shot} {Object} {Detection}},
	url = {https://arxiv.org/abs/2003.06957v1},
	abstract = {Detecting rare objects from a few examples is an emerging problem. Prior works show meta-learning is a promising approach. But, fine-tuning techniques have drawn scant attention. We find that fine-tuning only the last layer of existing detectors on rare classes is crucial to the few-shot object detection task. Such a simple approach outperforms the meta-learning methods by roughly 2{\textasciitilde}20 points on current benchmarks and sometimes even doubles the accuracy of the prior methods. However, the high variance in the few samples often leads to the unreliability of existing benchmarks. We revise the evaluation protocols by sampling multiple groups of training examples to obtain stable comparisons and build new benchmarks based on three datasets: PASCAL VOC, COCO and LVIS. Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks. The code as well as the pretrained models are available at https://github.com/ucbdrive/few-shot-object-detection.},
	language = {en},
	urldate = {2024-09-13},
	journal = {arXiv.org},
	author = {Wang, Xin and Huang, Thomas E. and Darrell, Trevor and Gonzalez, Joseph E. and Yu, Fisher},
	month = mar,
	year = {2020},
}

@misc{noauthor_sensors_2024,
	title = {Sensors {\textbar} {Free} {Full}-{Text} {\textbar} {Object} {Detection} {Techniques} {Applied} on {Mobile} {Robot} {Semantic} {Navigation}},
	url = {https://www.mdpi.com/1424-8220/14/4/6734},
	urldate = {2024-09-13},
	month = sep,
	year = {2024},
}

@inproceedings{kang_few-shot_2019,
	title = {Few-{Shot} {Object} {Detection} via {Feature} {Reweighting}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Kang_Few-Shot_Object_Detection_via_Feature_Reweighting_ICCV_2019_paper.html},
	urldate = {2024-09-13},
	author = {Kang, Bingyi and Liu, Zhuang and Wang, Xin and Yu, Fisher and Feng, Jiashi and Darrell, Trevor},
	year = {2019},
	pages = {8420--8429},
}

@article{kim_uav-assisted_2019,
	title = {{UAV}-assisted autonomous mobile robot navigation for as-is {3D} data collection and registration in cluttered environments},
	volume = {106},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580519301918},
	doi = {10.1016/j.autcon.2019.102918},
	abstract = {This paper discusses enabling autonomous mobile robots to operate in unstructured terrain and environments and gather data in a time-efficient manner. In rugged environments such as construction sites and disaster sites, spatiotemporal data is difficult to acquire since these environments are potentially hazardous to humans. Autonomous robot systems represent a reasonable solution to provide topographic and surveillance data that can assist human activity in these environments, whether for exploration, mapping, or search and rescue. However, automatically operating the mobile laser scanning robots at cluttered environments is challenging because as-is geometrical conditions of the site are difficult to comprehend from the ground level due to the blocked line-of-site views. To address the issue, this paper introduces a new framework for operating mobile robots equipped with a laser scanning system in cluttered outdoor environments with the aid of an unmanned aerial vehicle (UAV). To obtain an initial map from the current field, this method first deploys UAV to collect photographic images of a site and builds a point cloud of a3D terrain of the site including obstacle information. A voxel grid is then created from the UAV-generated point cloud, and simulation for laser scan planning is conducted to determine the stationary laser scan positions at which a mobile robot can collect data with less occluded views while capturing crucial geometric information as much as it can. Finally, optimal paths for the mobile robot to navigate among the estimated scan positions are generated. Promising test results were obtained from a real-world outdoor structural material test yard as an example of a cluttered environment. It is expected that the proposed UAV-assisted robotic approach can significantly reduce human intervention and time for data collection and processing and provide technologies to enable cluttered environments to be frequently monitored, updated, and analyzed to support timely decision-making.},
	urldate = {2024-09-13},
	journal = {Automation in Construction},
	author = {Kim, Pileun and Park, Jisoo and Cho, Yong K. and Kang, Junsuk},
	month = oct,
	year = {2019},
	keywords = {Laser scanning, Mobile robot navigation, Point cloud registration, Scan planning, Simultaneous localization and mapping (SLAM)},
	pages = {102918},
}

@misc{noauthor_airdet_2024,
	title = {{AirDet}},
	url = {https://jaraxxus-me.github.io/ECCV2022_AirDet/},
	urldate = {2024-09-13},
	month = sep,
	year = {2024},
}

@article{shashank_application_2022,
	series = {International {Conference} on {Intelligent} {Engineering} {Approach}({ICIEA}-2022)},
	title = {Application of few-shot object detection in robotic perception},
	volume = {3},
	issn = {2666-285X},
	url = {https://www.sciencedirect.com/science/article/pii/S2666285X22000607},
	doi = {10.1016/j.gltp.2022.04.024},
	abstract = {An object detection technique for robotic perception plays a vital role for robots to perform the task that it is functioned to do. In this paper, an efficient and accurate method for object detection for robots is proposed. The paper suggests implementing Few-shot object detection network for robotic vision using the Attention network and attention RPN module. The Multi-relation detector is used to compare two frames and eliminate negative objects from the frame which further enforces the suggested model. Using Contrastive training strategy, the robot is trained to exploit the resemblance between the few-shot support frame and query frame to detect the positive objects and eliminate the negative objects. This method is proposed to help robots perceive the object of interest to perform pick, place, and various other actions. This paper utilizes the COCO dataset to train the network which contains close to 1000 different categories. This method would help accelerate industry 4.0 and has potential in a wide range of applications.},
	number = {1},
	urldate = {2024-09-13},
	journal = {Global Transitions Proceedings},
	author = {Shashank, T. K. and Hitesh, N. and Gururaja, H. S.},
	month = jun,
	year = {2022},
	note = {Number: 1},
	keywords = {Attention RPN, Few-Shot learning, Robotic Perception, Robotic vision, multi-relation detector},
	pages = {114--118},
}

@inproceedings{srichitra_implementation_2022,
	address = {Singapore},
	title = {Implementation of {ROS}-{Based} {Mobile} {Robots} with {Few} {Shot} {Object} {Detection} {Using} {TensorFlow} {API}},
	isbn = {978-981-19070-7-4},
	doi = {10.1007/978-981-19-0707-4_42},
	abstract = {Existing ROS packages for object detection are based on traditional template matching techniques. They are used to identify only a few classes and cannot be adjusted to detect new classes. These methods also fail with changes in parameters like lighting, shape, size and orientation. Deep learning-based object detection can overcome these drawbacks and are also more accurate compared to traditional methods. Therefore, few shot object detection is attempted with the help of TensorFlow object detection API. The aim of this research is to integrate few shot object detection into a robotic application. This object detection method learns to detect objects from a few examples per class. This solves the problem of requirement of large datasets and also develops a ROS package that can easily use object detection models from TensorFlow object detection API. Thus, the integration can be deployed in any robotic application based on ROS framework.},
	language = {en},
	booktitle = {Soft {Computing}: {Theories} and {Applications}},
	publisher = {Springer Nature},
	author = {Srichitra, S. and Sreeja, S.},
	editor = {Kumar, Rajesh and Ahn, Chang Wook and Sharma, Tarun K. and Verma, Om Prakash and Agarwal, Anand},
	year = {2022},
	pages = {457--468},
}

@article{espinace_indoor_2013,
	title = {Indoor scene recognition by a mobile robot through adaptive object detection},
	volume = {61},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889013000821},
	doi = {10.1016/j.robot.2013.05.002},
	abstract = {Mobile robotics has achieved notable progress, however, to increase the complexity of the tasks that mobile robots can perform in natural environments, we need to provide them with a greater semantic understanding of their surrounding. In particular, identifying indoor scenes, such as an Office or a Kitchen, is a highly valuable perceptual ability for an indoor mobile robot, and in this paper we propose a new technique to achieve this goal. As a distinguishing feature, we use common objects, such as Doors or furniture, as a key intermediate representation to recognize indoor scenes. We frame our method as a generative probabilistic hierarchical model, where we use object category classifiers to associate low-level visual features to objects, and contextual relations to associate objects to scenes. The inherent semantic interpretation of common objects allows us to use rich sources of online data to populate the probabilistic terms of our model. In contrast to alternative computer vision based methods, we boost performance by exploiting the embedded and dynamic nature of a mobile robot. In particular, we increase detection accuracy and efficiency by using a 3D range sensor that allows us to implement a focus of attention mechanism based on geometric and structural information. Furthermore, we use concepts from information theory to propose an adaptive scheme that limits computational load by selectively guiding the search for informative objects. The operation of this scheme is facilitated by the dynamic nature of a mobile robot that is constantly changing its field of view. We test our approach using real data captured by a mobile robot navigating in Office and home environments. Our results indicate that the proposed approach outperforms several state-of-the-art techniques for scene recognition.},
	number = {9},
	urldate = {2024-09-13},
	journal = {Robotics and Autonomous Systems},
	author = {Espinace, P. and Kollar, T. and Roy, N. and Soto, A.},
	month = sep,
	year = {2013},
	note = {Number: 9},
	keywords = {3D cues, Mobile robotics, Scene recognition, Visual recognition},
	pages = {932--947},
}

@inproceedings{griffin_mobile_2023,
	title = {Mobile {Robot} {Manipulation} {Using} {Pure} {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content/WACV2023/html/Griffin_Mobile_Robot_Manipulation_Using_Pure_Object_Detection_WACV_2023_paper.html},
	language = {en},
	urldate = {2024-09-13},
	author = {Griffin, Brent},
	year = {2023},
	pages = {561--571},
}

@misc{fan_generalized_2021,
	title = {Generalized {Few}-{Shot} {Object} {Detection} without {Forgetting}},
	url = {http://arxiv.org/abs/2105.09491},
	doi = {10.48550/arXiv.2105.09491},
	abstract = {Recently few-shot object detection is widely adopted to deal with data-limited situations. While most previous works merely focus on the performance on few-shot categories, we claim that detecting all classes is crucial as test samples may contain any instances in realistic applications, which requires the few-shot detector to learn new concepts without forgetting. Through analysis on transfer learning based methods, some neglected but beneficial properties are utilized to design a simple yet effective few-shot detector, Retentive R-CNN. It consists of Bias-Balanced RPN to debias the pretrained RPN and Re-detector to find few-shot class objects without forgetting previous knowledge. Extensive experiments on few-shot detection benchmarks show that Retentive R-CNN significantly outperforms state-of-the-art methods on overall performance among all settings as it can achieve competitive results on few-shot classes and does not degrade the base class performance at all. Our approach has demonstrated that the long desired never-forgetting learner is available in object detection.},
	urldate = {2024-09-13},
	publisher = {arXiv},
	author = {Fan, Zhibo and Ma, Yuchen and Li, Zeming and Sun, Jian},
	month = may,
	year = {2021},
	note = {Issue: arXiv:2105.09491
arXiv:2105.09491 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hernandez_object_2016,
	title = {Object {Detection} {Applied} to {Indoor} {Environments} for {Mobile} {Robot} {Navigation}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/16/8/1180},
	doi = {10.3390/s16081180},
	abstract = {To move around the environment, human beings depend on sight more than their other senses, because it provides information about the size, shape, color and position of an object. The increasing interest in building autonomous mobile systems makes the detection and recognition of objects in indoor environments a very important and challenging task. In this work, a vision system to detect objects considering usual human environments, able to work on a real mobile robot, is developed. In the proposed system, the classification method used is Support Vector Machine (SVM) and as input to this system, RGB and depth images are used. Different segmentation techniques have been applied to each kind of object. Similarly, two alternatives to extract features of the objects are explored, based on geometric shape descriptors and bag of words. The experimental results have demonstrated the usefulness of the system for the detection and location of the objects in indoor environments. Furthermore, through the comparison of two proposed methods for extracting features, it has been determined which alternative offers better performance. The final results have been obtained taking into account the proposed problem and that the environment has not been changed, that is to say, the environment has not been altered to perform the tests.},
	language = {en},
	number = {8},
	urldate = {2024-09-13},
	journal = {Sensors},
	author = {Hernández, Alejandra Carolina and Gómez, Clara and Crespo, Jonathan and Barber, Ramón},
	month = aug,
	year = {2016},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Support Vector Machine, mobile robots, object classification, object detection, robot navigation, shapes descriptors},
	pages = {1180},
}

@article{hu_object_2022,
	title = {Object {Detection} {Algorithm} for {Wheeled} {Mobile} {Robot} {Based} on an {Improved} {YOLOv4}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/9/4769},
	doi = {10.3390/app12094769},
	abstract = {In practical applications, the intelligence of wheeled mobile robots is the trend of future development. Object detection for wheeled mobile robots requires not only the recognition of complex surroundings, but also the deployment of algorithms on resource-limited devices. However, the current state of basic vision technology is insufficient to meet demand. Based on this practical problem, in order to balance detection accuracy and detection efficiency, we propose an object detection algorithm based on a combination of improved YOLOv4 and improved GhostNet in this paper. Firstly, the backbone feature extraction network of original YOLOv4 is replaced with the trimmed GhostNet network. Secondly, enhanced feature extraction network in the YOLOv4, ordinary convolution is supplanted with a combination of depth-separable and ordinary convolution. Finally, the hyperparameter optimization was carried out. The experimental results show that the improved YOLOv4 network proposed in this paper has better object detection performance. Specifically, the precision, recall, F1, mAP (0.5) values, and mAP (0.75) values are 88.89\%, 87.12\%, 88.00\%, 86.84\%, and 50.91\%, respectively. Although the mAP (0.5) value is only 2.23\% less than the original YOLOv4, it is higher than YOLOv4\_tiny, Eifficientdet-d0, YOLOv5n, and YOLOv5 compared to 29.34\%, 28.99\%, 20.36\%, and 18.64\%, respectively. In addition, it outperformed YOLOv4 in terms of mAP (0.75) value and precision, and its model size is only 42.5 MB, a reduction of 82.58\% when compared to YOLOv4’s model size.},
	language = {en},
	number = {9},
	urldate = {2024-09-13},
	journal = {Applied Sciences},
	author = {Hu, Yanxin and Liu, Gang and Chen, Zhiyu and Guo, Jianwei},
	month = jan,
	year = {2022},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ghostNet, improved YOLOv4 network, object detection, wheeled mobile robots},
	pages = {4769},
}

@article{chiatti_task-agnostic_2020,
	title = {Task-{Agnostic} {Object} {Recognition} for {Mobile} {Robots} through {Few}-{Shot} {Image} {Matching}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/9/3/380},
	doi = {10.3390/electronics9030380},
	abstract = {To assist humans with their daily tasks, mobile robots are expected to navigate complex and dynamic environments, presenting unpredictable combinations of known and unknown objects. Most state-of-the-art object recognition methods are unsuitable for this scenario because they require that: (i) all target object classes are known beforehand, and (ii) a vast number of training examples is provided for each class. This evidence calls for novel methods to handle unknown object classes, for which fewer images are initially available (few-shot recognition). One way of tackling the problem is learning how to match novel objects to their most similar supporting example. Here, we compare different (shallow and deep) approaches to few-shot image matching on a novel data set, consisting of 2D views of common object types drawn from a combination of ShapeNet and Google. First, we assess if the similarity of objects learned from a combination of ShapeNet and Google can scale up to new object classes, i.e., categories unseen at training time. Furthermore, we show how normalising the learned embeddings can impact the generalisation abilities of the tested methods, in the context of two novel configurations: (i) where the weights of a Convolutional two-branch Network are imprinted and (ii) where the embeddings of a Convolutional Siamese Network are L2-normalised.},
	language = {en},
	number = {3},
	urldate = {2024-09-13},
	journal = {Electronics},
	author = {Chiatti, Agnese and Bardaro, Gianluca and Bastianelli, Emanuele and Tiddi, Ilaria and Mitra, Prasenjit and Motta, Enrico},
	month = mar,
	year = {2020},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {few-shot object recognition, image matching, robotics},
	pages = {380},
}

@misc{vasquez_irvingvasquezvpl_2024-1,
	title = {irvingvasquez/vpl},
	copyright = {BSD-3-Clause},
	url = {https://github.com/irvingvasquez/vpl},
	abstract = {View planning library},
	urldate = {2024-09-17},
	author = {Vasquez, Irving},
	month = jan,
	year = {2024},
	note = {original-date: 2017-01-20T01:01:18Z},
}

@article{vasquez-gomez_volumetric_2014,
	title = {Volumetric {Next}-best-view {Planning} for {3D} {Object} {Reconstruction} with {Positioning} {Error}},
	volume = {11},
	issn = {1729-8806},
	url = {https://doi.org/10.5772/58759},
	doi = {10.5772/58759},
	abstract = {Three-dimensional (3D) object reconstruction is the process of building a 3D model of a real object. This task is performed by taking several scans of an object from different locations (views). Due to the limited field of view of the sensor and the object's self-occlusions, it is a difficult problem to solve. In addition, sensor positioning by robots is not perfect, making the actual view different from the expected one. We propose a next best view (NBV) algorithm that determines each view to reconstruct an arbitrary object. Furthermore, we propose a method to deal with the uncertainty in sensor positioning. The algorithm fulfills all the constraints of a reconstruction process, such as new information, positioning constraints, sensing constraints and registration constraints. Moreover, it improves the scan's quality and reduces the navigation distance. The algorithm is based on a search-based paradigm where a set of candidate views is generated and then each candidate view is evaluated to determine which one is the best. To deal with positioning uncertainty, we propose a second stage which re-evaluates the views according to their neighbours, such that the best view is that which is within a region of the good views. The results of simulation and comparisons with previous approaches are presented.},
	language = {en},
	number = {10},
	urldate = {2024-09-17},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Vasquez-Gomez, J. Irving and Sucar, L. Enrique and Murrieta-Cid, Rafael and Lopez-Damian, Efrain},
	month = oct,
	year = {2014},
	note = {Publisher: SAGE Publications},
	keywords = {Important},
	pages = {159},
}

@article{krainin_manipulator_2011,
	title = {Manipulator and object tracking for in-hand {3D} object modeling},
	volume = {30},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364911403178},
	doi = {10.1177/0278364911403178},
	abstract = {Recognizing and manipulating objects is an important task for mobile robots performing useful services in everyday environments. While existing techniques for object recognition related to manipulation provide very good results even for noisy and incomplete data, they are typically trained using data generated in an offline process. As a result, they do not enable a robot to acquire new object models as it operates in an environment. In this paper we develop an approach to building 3D models of unknown objects based on a depth camera observing the robot’s hand while moving an object. The approach integrates both shape and appearance information into an articulated Iterative Closest Point approach to track the robot’s manipulator and the object. Objects are modeled by sets of surfels, which are small patches providing occlusion and appearance information. Experiments show that our approach provides very good 3D models even when the object is highly symmetric and lacks visual features and the manipulator motion is noisy. Autonomous object modeling represents a step toward improved semantic understanding, which will eventually enable robots to reason about their environments in terms of objects and their relations rather than through raw sensor data.},
	language = {en},
	number = {11},
	urldate = {2024-09-17},
	journal = {The International Journal of Robotics Research},
	author = {Krainin, Michael and Henry, Peter and Ren, Xiaofeng and Fox, Dieter},
	month = sep,
	year = {2011},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Manipulating for best view},
	pages = {1311--1327},
}

@article{velez_planning_2011,
	title = {Planning to {Perceive}: {Exploiting} {Mobility} for {Robust} {Object} {Detection}},
	volume = {21},
	issn = {2334-0843, 2334-0835},
	shorttitle = {Planning to {Perceive}},
	url = {https://ojs.aaai.org/index.php/ICAPS/article/view/13471},
	doi = {10.1609/icaps.v21i1.13471},
	abstract = {Consider the task of a mobile robot autonomously navigating through an environment while detecting and mapping objects of interest using a noisy object detector. The robot must reach its destination in a timely manner, but is rewarded for correctly detecting recognizable objects to be added to the map, and penalized for false alarms. However, detector performance typically varies with vantage point, so the robot beneﬁts from planning trajectories which maximize the efﬁcacy of the recognition system.},
	language = {en},
	urldate = {2024-09-17},
	journal = {Proceedings of the International Conference on Automated Planning and Scheduling},
	author = {Velez, Javier and Hemann, Garrett and Huang, Albert and Posner, Ingmar and Roy, Nicholas},
	month = mar,
	year = {2011},
	note = {Publisher: Association for the Advancement of Artificial Intelligence (AAAI)},
	pages = {266--273},
}

@inproceedings{vasquez_next-best-view_2011,
	address = {Berlin, Heidelberg},
	title = {Next-{Best}-{View} {Planning} for {3D} {Object} {Reconstruction} under {Positioning} {Error}},
	isbn = {978-3-642-25324-9},
	doi = {10.1007/978-3-642-25324-9_37},
	abstract = {To acquire a 3D model of an object it is necessary to plan a set of locations, called views, where a range sensor will be placed. The problem is solved in greedy manner, by selecting iteratively next-best-views. When a mobile robot is used, we have to take into account positioning errors, given that they can affect the quality and efficiency of the plan. We propose a method to plan “safe views” which are successful even when there is positioning error. The method is based on a reevaluation of the candidate views according to their neighbors, so view points which are safer against positioning error are preferred. The method was tested in simulation with objects of different complexities. Experimental results show that the proposed method achieves similar results as the ideal case without error, reducing the number of views required against the standard approach that does not consider positioning error.},
	language = {en},
	booktitle = {Advances in {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Vásquez, Juan Irving and Sucar, L. Enrique},
	editor = {Batyrshin, Ildar and Sidorov, Grigori},
	year = {2011},
	keywords = {Important, Modeling, Next-Best-View, Object Reconstruction, Planning, View Planning},
	pages = {429--442},
}

@misc{torabi_integrated_2011,
	title = {Integrated view and path planning for a fully autonomous mobile-manipulator system for {3D} object modeling},
	copyright = {Copyright is held by the author.},
	url = {https://summit.sfu.ca/item/12113},
	abstract = {We have designed and implemented a fully autonomous system for building a 3D model of an object in situ. Our system assumes no knowledge of object other than that it is within a bounding box whose location and size are known a priori, and furthermore, the environment is unknown. The system consists of a mobile manipulator, a powerbot mobile base with a six degrees of freedom (DOF) powercube arm mounted on it. The arm and the powerbot are equipped with line-scan range sensors, which provide range images that are used to build the work space Octree model and the object point cloud model. The object modeling system is comprised of three broad modules: (i) 3D model construction, (ii) a modeling view planner, which determines the next scanning pose for modeling, and (iii) a path planner, which determines a collision-free path to move the mobile-manipulator to a desired pose. Our research focuses on automating this process, which concerns the last two modules. Our modeling view planner calculates the target areas to be scanned, and then efficiently searches the five-dimensional viewpoint space to determine the best viewpoint for scanning the target areas. The path planning module itself consists of two subproblems: (a) exploration view planning, and (b) basic path planning. The former concerns developing an exploration strategy that facilitates manoeuvering the mobile-manipulator, for which we provide a formulation of C-space entropy reduction for range sensors for occupancy grid maps. For the basic path planning, a probabilistic Roadmap method (SBIC-PRM) is used for moving the mobile-manipulator to the view configuration. To construct a complete autonomous 3D modeling system, all three modules are repeatedly solved in an interleaved fashion. The process of modeling continues until a formally proved termination criteria is satisfied. We present extensive experimental results with our 3D object modeling system running on this real test-bed. The robot is started in unknown environments, and builds the object model in less than 35 scans. Each iteration takes about 4 minutes. The experimental results show the ability and efficacy of the system both in modeling the object and in the required exploration of the environment.},
	language = {en},
	urldate = {2024-09-17},
	author = {Torabi, Liila},
	month = sep,
	year = {2011},
	note = {Publisher: Simon Fraser University},
}
